{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Techniques in Flappy Bird\n",
    "\n",
    "Reinforcement learning has been the foundation of breakthroughs in go, chess, and protein folding [^1][^2][^3]. It works by placing neural networks in a simulated environment and rewarding them for good behavior. \n",
    "\n",
    "I apply reinforcement learning to the mobile game flappy bird[^4]. Flappy bird is a simple mobile game that uses a discrete state space and a continuous action space, making it a good introduction to reinforcement learning. I start with extremely easy state representations and reward functions, and gradually increase the complexity. Growing the problem size as I succeed at each step provides prerequisite intuition to debug issues that occur as networks become more complex.\n",
    "\n",
    "[^1]: [Mastering the Game of Go without Human Knowledge](https://discovery.ucl.ac.uk/id/eprint/10045895/1/agz_unformatted_nature.pdf)\\\n",
    "[^2]: [Mastering Chess and Shogi by Planning with a Tree Search](https://arxiv.org/pdf/1712.01815.pdf)\\\n",
    "[^3]: [Highly Accurate Protein Structure Prediction with AlphaFold](https://www.nature.com/articles/s41586-021-03819-2)\\\n",
    "[^4]: [Flappy Bird](https://flappy-bird.co)\\\n",
    "[^5]: This is a continuation of a project that I started two years ago, a final project for a deep learning course. I didn't end up getting it working then, but have been working on it since."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Reinforcement Learning\n",
    "In reinforcement learning, an agent lives in an environment specified by an event loop. The agent must observe the relationship between its actions and the environment's response. The environment's response consists of two things: (1) a reward signal, and (2) a new state.\n",
    "\n",
    "``` python\n",
    "observation = env.get_initial_state()\n",
    "while True:\n",
    "    action = agent.act(observation)\n",
    "    observation, reward = env.step(action)\n",
    "    agent.learn(observation, reward)\n",
    "```\n",
    "Formally, the environment is defined by four things: (1) the state space $S$, (2) the action space $A$, (3) the transition function $P$, and (4) the reward function $R$.\n",
    "\n",
    "- The state space $S$ is the set of all possible states the agent can observe. \n",
    "- The action space $A$ is the set of all possible actions the agent can take.\n",
    "- The transition function $P(s'|s,a) \\rightarrow [0,1]$ is the probability of transitioning from state $s \\in S$ to state $s' \\in S$ given action $a \\in A$. \n",
    "- The reward function $R(s,a,s') \\rightarrow \\mathbb{R}$ is the reward the agent receives for transitioning from state $s$ to state $s'$.\n",
    "\n",
    "When the agent interacts with the environment, its interaction can be described as a sequence of states, actions, and rewards:\n",
    "\n",
    "$$\n",
    "(s_0, a_0, r_0, s_1, a_1, r_1, \\cdots)\n",
    "$$\n",
    "\n",
    "[^6]: [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/RLbook2020.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flappy Bird Environment\n",
    "I used the flappy bird gymnasium package to create the environment. \n",
    "The Flappy Bird environment is characterized by a state space, an action space, and a reward function. \n",
    "1. The state space $S \\in \\mathbb{R}^{12}$ is a vector of 12 numbers, representing various distances, velocities, and angles related to the bird and the pipes. See [^8] for complete details. \n",
    "2. The action space $A = \\{0, 1\\}$ allows the agent to either \"do nothing\" or \"flap\". \n",
    "3. The transition probability function $P$ is unknown, and the agent must learn it. If $P$ was known prior, then we could compute the optimal policy trivially with the Bellman Equation [^9]. \n",
    "4. The reward function $R$ provides feedback based on the agent's state transitions, rewarding survival, successful pipe navigation, and penalizing death. The reward function is defined as:\n",
    "\n",
    " $R(s, a) = \\begin{cases} +0.1 & \\text{if the agent is alive} \\\\ +1.0 & \\text{if the agent passes through a pipe} \\\\ -1.0 & \\text{if the agent dies} \\end{cases}$\n",
    "\n",
    "\n",
    "[^7]: [Flappy Bird Gymnasium](https://github.com/Kautenja/flappy-bird-gymnasium)\\\n",
    "[^8]: [Flappy Bird State Vector](https://github.com/markub3327/flappy-bird-gymnasium)\\\n",
    "[^9]: Reinforcement learning: an introduction Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check for the Flappy Bird Environment\n",
    "\n",
    "Before exploring reinforcement learning techniques, it is important to sanity check the flappy bird environment. Sanity testing the environment will make us more knowledgeable when debugging future issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flappy_bird_gymnasium\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from agent import Agent\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import Video\n",
    "\n",
    "# Create the environment\n",
    "env = gymnasium.make(\"FlappyBird-v0\", \n",
    "                     render_mode=\"rgb_array\",\n",
    "                     use_lidar=False,\n",
    "                     normalize_obs=True)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "\n",
    "video_name = 'sanity_check0.mp4'\n",
    "need_to_create_video = not os.path.exists(video_name)\n",
    "if need_to_create_video:\n",
    "    out = cv2.VideoWriter(video_name, fourcc, 30.0, (288, 512))\n",
    "\n",
    "observations = np.zeros((0, 12))\n",
    "env.reset()\n",
    "while True:\n",
    "    obs, reward, terminal, _, _ = env.step(env.action_space.sample())\n",
    "    observations = np.vstack((observations, obs))\n",
    "    image = env.render()\n",
    "    \n",
    "    # Convert the image from RGB to BGR\n",
    "    image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Write the frame to the video file\n",
    "    if need_to_create_video:\n",
    "        out.write(image_bgr)\n",
    "    \n",
    "    if terminal:\n",
    "        break    \n",
    "\n",
    "# Release resources\n",
    "env.close()\n",
    "out.release()\n",
    "\n",
    "# Display the converted video in the notebook\n",
    "Video(video_name, embed=True)\n",
    "print(observations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sanity check confirms that the state space is 12-dimensional, and that the environment renders as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward\n",
    "The agent's goal is to maximize its return, the cumulative reward over time. The expected return is\n",
    "$$\\sum_{k=0}^\\infty\\gamma^kR_{t+k+1}$$\n",
    "where $\\gamma$ is the discount rate, a penalty applied to future rewards because they are less certain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "In Q-Learning approximates a function $Q(s,a) \\rightarrow \\mathbb{R}, \\text{where }s \\in \\mathcal{S}, a \\in \\mathcal{A}$ to achieve the highest return in a trajectory. \n",
    "$$\n",
    "Q(s,a) = Q(s,a) + \\alpha[R_t + \\gamma(\\argmax_{a}(Q(s',a)) - Q(s,a))]\n",
    "$$\n",
    "\n",
    "To start, I handcrafted a far simpler version of the state space in flappy bird where:\n",
    " $$State = \\begin{cases} 1 & \\text{jumping is optimal} \\\\\n",
    "  0 & \\text{not jumping is optimal} \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "This handcrafted reward system is not perfect, but results in significantly better performance than the other case. An optimal agent would perform similarly, but not identically to this model. The handcrafted representation makes evaluating whether future models simpler by comparing the future models policy to the handcrafted policy. Because Q(s,a) naturally involves some exploration, exploration must be turned off before evaluating the quality of the function.\n",
    "### Untrained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\n",
      "[[0.00179114 0.00010783]\n",
      " [0.00182666 0.00011814]]\n",
      "Episode 200\n",
      "[[0.00301974 0.00018332]\n",
      " [0.00391069 0.00022874]]\n",
      "Episode 300\n",
      "[[0.00450351 0.00027342]\n",
      " [0.00576536 0.00034599]]\n",
      "Episode 400\n",
      "[[0.00576136 0.00033854]\n",
      " [0.00781108 0.00048725]]\n",
      "Episode 500\n",
      "[[0.00726559 0.00040241]\n",
      " [0.00968072 0.00060255]]\n",
      "Episode 600\n",
      "[[0.00862435 0.00047898]\n",
      " [0.01168487 0.00073105]]\n",
      "Episode 700\n",
      "[[0.00978769 0.00054696]\n",
      " [0.01365629 0.00085106]]\n",
      "Episode 800\n",
      "[[0.01128677 0.00062552]\n",
      " [0.01556837 0.00102541]]\n",
      "Episode 900\n",
      "[[0.01260919 0.00068268]\n",
      " [0.01749984 0.00117636]]\n",
      "Episode 1000\n",
      "[[0.01420174 0.00076953]\n",
      " [0.01936488 0.00134533]]\n",
      "Episode 1100\n",
      "[[0.01551394 0.00079117]\n",
      " [0.02135159 0.00151054]]\n",
      "Episode 1200\n",
      "[[0.01710656 0.00088392]\n",
      " [0.02318762 0.00166535]]\n",
      "Episode 1300\n",
      "[[0.01835271 0.00096194]\n",
      " [0.02531136 0.001835  ]]\n",
      "Episode 1400\n",
      "[[0.02011271 0.00102741]\n",
      " [0.02699202 0.00202197]]\n",
      "Episode 1500\n",
      "[[0.02180144 0.00114869]\n",
      " [0.02871099 0.00219662]]\n",
      "Episode 1600\n",
      "[[0.02340928 0.00123996]\n",
      " [0.03043922 0.00238049]]\n",
      "Episode 1700\n",
      "[[0.02499243 0.00132995]\n",
      " [0.03223193 0.00254083]]\n",
      "Episode 1800\n",
      "[[0.02611424 0.0013927 ]\n",
      " [0.03422664 0.00271825]]\n",
      "Episode 1900\n",
      "[[0.02744435 0.00147104]\n",
      " [0.0360233  0.00289247]]\n",
      "Episode 2000\n",
      "[[0.0287971  0.00155002]\n",
      " [0.03800014 0.00305259]]\n",
      "Episode 2100\n",
      "[[0.03031594 0.00165866]\n",
      " [0.03979025 0.00322918]]\n",
      "Episode 2200\n",
      "[[0.03171752 0.00172942]\n",
      " [0.04152463 0.00339827]]\n",
      "Episode 2300\n",
      "[[0.03319959 0.0018152 ]\n",
      " [0.04336347 0.00356477]]\n",
      "Episode 2400\n",
      "[[0.03444261 0.00190162]\n",
      " [0.04534344 0.00372663]]\n",
      "Episode 2500\n",
      "[[0.03566039 0.00198185]\n",
      " [0.04716119 0.00389744]]\n",
      "Episode 2600\n",
      "[[0.03693317 0.00205235]\n",
      " [0.04898593 0.00408634]]\n",
      "Episode 2700\n",
      "[[0.0383648  0.00215094]\n",
      " [0.05069724 0.00429407]]\n",
      "Episode 2800\n",
      "[[0.03953603 0.00221935]\n",
      " [0.05253825 0.00448543]]\n",
      "Episode 2900\n",
      "[[0.04080633 0.0022957 ]\n",
      " [0.0543738  0.00466201]]\n",
      "Episode 3000\n",
      "[[0.04214086 0.00238639]\n",
      " [0.05621379 0.00486387]]\n",
      "Episode 3100\n",
      "[[0.04350328 0.00248611]\n",
      " [0.05792771 0.00507158]]\n",
      "Episode 3200\n",
      "[[0.04485668 0.00258946]\n",
      " [0.05957017 0.00527187]]\n",
      "Episode 3300\n",
      "[[0.04657602 0.00268373]\n",
      " [0.06122104 0.00546567]]\n",
      "Episode 3400\n",
      "[[0.04790802 0.0027741 ]\n",
      " [0.06300958 0.00571064]]\n",
      "Episode 3500\n",
      "[[0.04940319 0.00291701]\n",
      " [0.06473957 0.00588883]]\n",
      "Episode 3600\n",
      "[[0.05052791 0.00300575]\n",
      " [0.06661888 0.00611024]]\n",
      "Episode 3700\n",
      "[[0.05217518 0.00313201]\n",
      " [0.06813582 0.00631859]]\n",
      "Episode 3800\n",
      "[[0.05371003 0.00322341]\n",
      " [0.06979442 0.00650965]]\n",
      "Episode 3900\n",
      "[[0.05529319 0.00334623]\n",
      " [0.07114544 0.00666875]]\n",
      "Episode 4000\n",
      "[[0.05660915 0.00347876]\n",
      " [0.07280774 0.00684624]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     state \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mdiscretize_state(current_obs)\n\u001b[0;32m---> 23\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     next_obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     26\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mdiscretize_state(next_obs)\n",
      "File \u001b[0;32m~/Documents/flappy-rl/agent.py:51\u001b[0m, in \u001b[0;36mAgent.act\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon:\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m) \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import flappy_bird_gymnasium\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from agent import Agent\n",
    "\n",
    "env = gymnasium.make(\"FlappyBird-v0\", \n",
    "                    # render_mode=\"human\",\n",
    "                    use_lidar=False,\n",
    "                    normalize_obs=True)\n",
    "agent = Agent()\n",
    "\n",
    "for i in range(int(1e6)):\n",
    "    current_obs, _ = env.reset()\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"Episode {i+1}\")\n",
    "        print(agent.qtable)\n",
    "    act = agent.demo_act if (i+1) % 1000 == 0 else agent.act\n",
    "    act = agent.demo_act\n",
    "    while True:\n",
    "        state = agent.discretize_state(current_obs)\n",
    "\n",
    "        action = agent.act(state)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        next_state = agent.discretize_state(next_obs)\n",
    "        agent.learn(state, action, reward, next_state, terminated)\n",
    "        \n",
    "        if terminated:\n",
    "            break \n",
    "        current_obs = next_obs\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Agent after 1.5 million iterations\n",
    "\n",
    "In this game with simplified state, the agent's learning process was extremely inefficient. It required 1.5 million iterations to learn the optimal jumping policy, which is surprising because the game has two discrete states and two discrete actions. The reason the agent took so long to converge was because it could only learn by measuring the payoff after an $\\epsilon$-greedy exploration. This work can be extended by measuring how well this does for different values of $\\epsilon$ and how policy-based methods would do a better job of learning the optimal policy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
