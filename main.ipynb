{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Techniques in Flappy Bird\n",
    "\n",
    "Reinforcement learning has been the foundation of breakthroughs in go, chess, and protein folding [^1][^2][^3]. It works by using neural networks to in a simulated environment and rewarding it for good behavior. Here, I apply reinforcement learning to the mobile game flappy bird[^4].\n",
    "\n",
    "Continuation of a project that I started two years ago, a final project for a deep learning course. I didn't end up getting it working then, but have been working on it since.\n",
    "\n",
    "[^1]: [Mastering the Game of Go without Human Knowledge](https://discovery.ucl.ac.uk/id/eprint/10045895/1/agz_unformatted_nature.pdf)\\\n",
    "[^2]: [Mastering Chess and Shogi by Planning with a Tree Search](https://arxiv.org/pdf/1712.01815.pdf)\\\n",
    "[^3]: [Highly Accurate Protein Structure Prediction with AlphaFold](https://www.nature.com/articles/s41586-021-03819-2)\\\n",
    "[^4]: [Flappy Bird](https://flappy-bird.co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Reinforcement Learning\n",
    "In reinforcement learning, an agent lives in an environment specified by an event loop. The agent must observe the relationship between its actions and the environment's response. The environment's response consists of two things: (1) a reward signal, and (2) a new state.\n",
    "\n",
    "``` python\n",
    "observation = env.get_initial_state()\n",
    "while True:\n",
    "    action = agent.act(observation)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    agent.learn(observation, reward)\n",
    "```\n",
    "Formally, the environment is defined by four things: (1) the state space $S$, (2) the action space $A$, (3) the transition function $P$, and (4) the reward function $R$.\n",
    "\n",
    "- The state space $S$ is the set of all possible states the agent can observe. \n",
    "- The action space $A$ is the set of all possible actions the agent can take.\n",
    "- The transition function $P(s'|s,a) \\rightarrow [0,1]$ is the probability of transitioning from state $s \\in S$ to state $s' \\in S$ given action $a \\in A$. \n",
    "- The reward function $R(s,a) \\rightarrow \\mathbb{R}$ is the reward the agent receives for transitioning from state $s$ to state $s'$.\n",
    "\n",
    "When the agent interacts with the environment, its interaction can be described as a sequence of states, actions, and rewards:\n",
    "\n",
    "$$\n",
    "(s_0, a_0, r_0, s_1, a_1, r_1, \\cdots)\n",
    "$$\n",
    "\n",
    "[^6]: [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/RLbook2020.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flappy Bird Environment\n",
    "I used the flappy bird gymnasium package to create the environment. \n",
    "The Flappy Bird environment is characterized by a state space, an action space, and a reward function. \n",
    "1. The state space $S \\in \\mathbb{R}^{12}$ is a vector of 12 numbers, representing various distances, velocities, and angles related to the bird and the pipes [^8]. \n",
    "2. The action space $A = \\{0, 1\\}$ allows the agent to either \"do nothing\" or \"flap\". \n",
    "3. The transition probability function $P$ is unknown, and the agent must learn it.\n",
    "4. The reward function $R$ provides feedback based on the agent's state transitions, rewarding survival, successful pipe navigation, and penalizing death. The reward function is defined as:\n",
    "\n",
    " $R(s, a, s') = \\begin{cases} +0.1 & \\text{if the agent is alive} \\\\ +1.0 & \\text{if the agent passes through a pipe} \\\\ -1.0 & \\text{if the agent dies} \\end{cases}$\n",
    "\n",
    "\n",
    "[^7]: [Flappy Bird Gymnasium](https://github.com/Kautenja/flappy-bird-gymnasium)\\\n",
    "[^8]: [Flappy Bird State Vector](https://github.com/markub3327/flappy-bird-gymnasium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check for the Flappy Bird Environment\n",
    "\n",
    "Before exploring reinforcement learning techniques, it is important to sanity check the flappy bird environment. Sanity testing the environment will make us more knowledgeable when debugging future issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flappy_bird_gymnasium\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from agent import Agent\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 12)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import Video\n",
    "\n",
    "# Create the environment\n",
    "env = gymnasium.make(\"FlappyBird-v0\", \n",
    "                     render_mode=\"rgb_array\",\n",
    "                     use_lidar=False,\n",
    "                     normalize_obs=True)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "\n",
    "need_to_create_video = True\n",
    "if need_to_create_video:\n",
    "    out = cv2.VideoWriter('flappy_bird.mp4', fourcc, 30.0, (288, 512))\n",
    "\n",
    "observations = np.zeros((0, 12))\n",
    "env.reset()\n",
    "while True:\n",
    "    obs, reward, terminal, _, _ = env.step(env.action_space.sample())\n",
    "    observations = np.vstack((observations, obs))\n",
    "    image = env.render()\n",
    "    \n",
    "    # Convert the image from RGB to BGR\n",
    "    image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Write the frame to the video file\n",
    "    if need_to_create_video:\n",
    "        out.write(image_bgr)\n",
    "    \n",
    "    if terminal:\n",
    "        break    \n",
    "\n",
    "# Release resources\n",
    "env.close()\n",
    "out.release()\n",
    "\n",
    "# Display the converted video in the notebook\n",
    "# Video(\"flappy_bird.mp4\", embed=True)\n",
    "print(observations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sanity check confirms that the state space is 12-dimensional, and that the environment renders as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
